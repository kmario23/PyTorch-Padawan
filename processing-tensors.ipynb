{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "$\\color{brown}{Preamble}$: At the time of this writing, I'm using **PyTorch** **`v1.7.1`** binded with **`cuda11.0`** and **`cudnn8.0`**."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import torch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"version: \", torch.__version__)\n",
    "mydevice = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "print(\"device : \", mydevice)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Maximum and Minimum Values"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "$\\color{red}{Question}$: Finding the positions (i.e. indices) of `n` maximum values in a tensor.  \n",
    "\n",
    "**Input**:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# for reproducibility\n",
    "myseed = 23\n",
    "torch.manual_seed(myseed)\n",
    "\n",
    "# sample tensor to work with\n",
    "t = torch.randint(low=0, high=50, size=(12,))\n",
    "print(t)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "$\\color{green}{Ans}$:  \n",
    "Given the above tensor `t`, now we have to find the positions in the tensor `t` that corresponds to the `howmany_max` maximum values."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "howmany_max = 5\n",
    "doublen = \"\\n\\n\"\n",
    "\n",
    "# gives the indices of values in increasing (i.e. ascending) order\n",
    "idxs_asc = t.argsort()\n",
    "\n",
    "# since we want the indices of maximum values, let's use the boolean flag `descending=True`\n",
    "idxs_desc = t.argsort(descending=True)\n",
    "print(\"indices: \", idxs_desc[:howmany_max])\n",
    "\n",
    "# alternatively, use this:\n",
    "# idxs_desc = torch.flip(t.argsort(), dims=(-1,))\n",
    "\n",
    "# get the values by indexing into the original tensor\n",
    "max_vals = t[idxs_desc[:howmany_max]]\n",
    "print(\"values: \", max_vals, end=doublen)\n",
    "\n",
    "# or simply use torch.sort() with the boolean flag `descending=True`\n",
    "sorted_t, max_idxs = torch.sort(t, descending=True)\n",
    "\n",
    "# access only `howmany_max` elements\n",
    "max_idxs[:howmany_max], sorted_t[:howmany_max]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "As we can see from the above result, we get duplicate values in the result tensor. This is because our original tensor contains duplicate entries. In such scenarios, PyTorch `argsort` randomly orders the positions (indices) of duplicated entries, both when using `descending=True` and `descending=False`."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Analogously, we can obtain the minimum values in a tensor simply by ignoring the `descending=True` flag."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Promotion of Tensors"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "$\\color{red}{Question}$: What are the ways to promote a tensor?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "$\\color{green}{Ans}$: When we introduce new axes (or dimensions) into the tensors for the purpose of broadcasting or some other operations, then this action is known as **_promotion_** of tensors.  \n",
    "For instance,"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "atensor = torch.randn(2, 3)\n",
    "print(\"original shape: \", atensor.shape, end=doublen)\n",
    "\n",
    "# NumPy style\n",
    "promoted = atensor[np.newaxis, ...]\n",
    "\n",
    "# barebones approach\n",
    "promoted = atensor[None, ...]\n",
    "\n",
    "# respecting torch API\n",
    "promoted = torch.unsqueeze(atensor, dim=0)\n",
    "promoted = atensor.unsqueeze(dim=0)    # unsqueeze_ for in-place modification\n",
    "\n",
    "print(\"promoted shape: \", promoted.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "As we observe from the above result, all these methods are equivalent and yield the same result. Hence we have the liberty to use whichever style we like the most."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "-----------------------"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
